# Multi-Task-Transformer-with-Drug-Specific-Attention-for-HIV-1-Protease-Inhibitor-Resistance
A transformer-based deep learning framework that processes  complete HIV-1 protease sequences (99 amino acids) to predict resistance to  eight protease inhibitors. Our model incorporates positional encoding and  drug-specific attention mechanisms to identify mutation patterns associated  with resistance.

The model achieved excellent prediction performance (RÂ² > 0.69) for six protease inhibitors (FPV, IDV, ATV, LPV, NFV, TPV) using a dataset of 4,512 clinical sequences. Attention analysis revealed that the model automatically identified known resistance positions including 46, 54, 82, 84, and 90, demonstrating biological interpretability. The multi-task 
architecture enabled efficient learning across drugs while maintaining drug-specific sensitivity patterns.



